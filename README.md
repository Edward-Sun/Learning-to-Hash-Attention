# Learning-to-Hash-Attention

See ["Sparse Attention with Learning to Hash"](https://openreview.net/forum?id=VGnOJhd5Q1q) (ICLR 2022) for the paper associated with this library.

The code is adapted from [Long-Range-Arena](https://github.com/google-research/long-range-arena) and [Transformer-XL](https://github.com/kimiyoung/transformer-xl). We provide the implementation of standard Transformer, Reformer, Performer, and LHA Transformer for encoder-only (LRA) and decoder-only (language modeling) tasks.

If you found this codebase useful, please consider citing the paper:

```
@inproceedings{
    sun2022sparse,
    title={Sparse Attention with Learning to Hash},
    author={Zhiqing Sun and Yiming Yang and Shinjae Yoo},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=VGnOJhd5Q1q}
}
```
